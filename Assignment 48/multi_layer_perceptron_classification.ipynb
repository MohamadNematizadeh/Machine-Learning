{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1257, 64), (1257, 10), (540, 64), (540, 10))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data\n",
    "Y = digits.target\n",
    "Y = np.eye(10)[Y]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "\n",
    "def cross_entropy_error(Y_pred, Y_gt):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(Y_gt * np.log(Y_pred + delta))\n",
    "\n",
    "\n",
    "def root_mean_squired_error(Y_pred, Y_gt):\n",
    "    return np.sqrt(np.mean((Y_pred - Y_gt) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = X_train.shape[1]\n",
    "H1 = 128\n",
    "H2 = 32\n",
    "D_out = Y_train.shape[1]\n",
    "\n",
    "Î· = 0.001\n",
    "epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, W3 = np.random.randn(D_in, H1), np.random.randn(H1, H2), np.random.randn(H2, D_out)\n",
    "B1, B2, B3 = np.random.randn(H1), np.random.randn(H2), np.random.randn(D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss train: 0.33665860267570585 acc train: 0.12649164677804295\n",
      "loss test: 0.30698953419289615 acc test: 0.20925925925925926\n",
      "loss train: 0.2995877859015418 acc train: 0.24025457438345266\n",
      "loss test: 0.2841633887673266 acc test: 0.32592592592592595\n",
      "loss train: 0.27964193997460546 acc train: 0.35481304693715193\n",
      "loss test: 0.2713646639158149 acc test: 0.4166666666666667\n",
      "loss train: 0.2670887913463017 acc train: 0.45664280031821797\n",
      "loss test: 0.2618874412460695 acc test: 0.48333333333333334\n",
      "loss train: 0.2552217025293206 acc train: 0.522673031026253\n",
      "loss test: 0.2529368847425654 acc test: 0.5277777777777778\n",
      "loss train: 0.24338587089569963 acc train: 0.5934765314240255\n",
      "loss test: 0.2443196284929653 acc test: 0.5703703703703704\n",
      "loss train: 0.23159864334249036 acc train: 0.6499602227525855\n",
      "loss test: 0.23568296639471548 acc test: 0.6129629629629629\n",
      "loss train: 0.22076756850047727 acc train: 0.6833731105807478\n",
      "loss test: 0.22779593640132734 acc test: 0.6388888888888888\n",
      "loss train: 0.2105924523035046 acc train: 0.7191726332537789\n",
      "loss test: 0.22081469714490545 acc test: 0.65\n",
      "loss train: 0.20053709829621053 acc train: 0.7501988862370724\n",
      "loss test: 0.2140543225339985 acc test: 0.6703703703703704\n",
      "loss train: 0.19184622155622288 acc train: 0.7788385043754972\n",
      "loss test: 0.20769943632968557 acc test: 0.6981481481481482\n",
      "loss train: 0.1841879365985849 acc train: 0.7979315831344471\n",
      "loss test: 0.20242258343907424 acc test: 0.7222222222222222\n",
      "loss train: 0.17779275816235357 acc train: 0.8162291169451074\n",
      "loss test: 0.1982984723084786 acc test: 0.7388888888888889\n",
      "loss train: 0.17202085662305372 acc train: 0.8265712012728719\n",
      "loss test: 0.19441348101217443 acc test: 0.7481481481481481\n",
      "loss train: 0.16666322497336042 acc train: 0.8337311058074781\n",
      "loss test: 0.19083811919060192 acc test: 0.7555555555555555\n",
      "loss train: 0.16175221683348995 acc train: 0.847255369928401\n",
      "loss test: 0.18758032635393723 acc test: 0.7685185185185185\n",
      "loss train: 0.15730784769757125 acc train: 0.8568019093078759\n",
      "loss test: 0.1847060852737994 acc test: 0.7796296296296297\n",
      "loss train: 0.1531604921637393 acc train: 0.86793953858393\n",
      "loss test: 0.18205720469247158 acc test: 0.7777777777777778\n",
      "loss train: 0.14925174473851954 acc train: 0.8750994431185362\n",
      "loss test: 0.17955181213131105 acc test: 0.7777777777777778\n",
      "loss train: 0.14548511751747065 acc train: 0.8822593476531424\n",
      "loss test: 0.17728587457148312 acc test: 0.7888888888888889\n",
      "loss train: 0.14191545651764267 acc train: 0.888623707239459\n",
      "loss test: 0.17512024881858704 acc test: 0.7962962962962963\n",
      "loss train: 0.13853693196969177 acc train: 0.8957836117740652\n",
      "loss test: 0.1730597804493611 acc test: 0.7944444444444444\n",
      "loss train: 0.13527494564712045 acc train: 0.898170246618934\n",
      "loss test: 0.17118196949149345 acc test: 0.8018518518518518\n",
      "loss train: 0.13217845295581498 acc train: 0.9021479713603818\n",
      "loss test: 0.16941458550356878 acc test: 0.8055555555555556\n",
      "loss train: 0.12928123372265587 acc train: 0.9061256961018298\n",
      "loss test: 0.16774918082699025 acc test: 0.8092592592592592\n",
      "loss train: 0.12656582354903012 acc train: 0.9101034208432777\n",
      "loss test: 0.166165599461021 acc test: 0.8111111111111111\n",
      "loss train: 0.12400940315924072 acc train: 0.9116945107398569\n",
      "loss test: 0.16461493223239626 acc test: 0.812962962962963\n",
      "loss train: 0.12160533720326351 acc train: 0.9124900556881463\n",
      "loss test: 0.16314635442170766 acc test: 0.8148148148148148\n",
      "loss train: 0.11935478940012208 acc train: 0.9156722354813047\n",
      "loss test: 0.16178608242440276 acc test: 0.8166666666666667\n",
      "loss train: 0.1172314455083403 acc train: 0.9220365950676214\n",
      "loss test: 0.16058398086080458 acc test: 0.8203703703703704\n",
      "loss train: 0.11519869326722675 acc train: 0.92442322991249\n",
      "loss test: 0.1594714989475305 acc test: 0.8240740740740741\n",
      "loss train: 0.11322287016838505 acc train: 0.9299920445505171\n",
      "loss test: 0.15838845034772345 acc test: 0.825925925925926\n",
      "loss train: 0.11131146302722296 acc train: 0.9315831344470963\n",
      "loss test: 0.1573751134479258 acc test: 0.825925925925926\n",
      "loss train: 0.10941535985419655 acc train: 0.933969769291965\n",
      "loss test: 0.15636746254512957 acc test: 0.8277777777777777\n",
      "loss train: 0.10752548632350946 acc train: 0.9371519490851233\n",
      "loss test: 0.15542087353634046 acc test: 0.8296296296296296\n",
      "loss train: 0.10578121971936874 acc train: 0.9387430389817024\n",
      "loss test: 0.15453863137152477 acc test: 0.8314814814814815\n",
      "loss train: 0.10415046707970729 acc train: 0.94351630867144\n",
      "loss test: 0.1536841139216421 acc test: 0.8333333333333334\n",
      "loss train: 0.10258542496162555 acc train: 0.9459029435163087\n",
      "loss test: 0.1528289872743922 acc test: 0.8351851851851851\n",
      "loss train: 0.10108115734860076 acc train: 0.9498806682577565\n",
      "loss test: 0.15196009952285902 acc test: 0.837037037037037\n",
      "loss train: 0.09964281158156126 acc train: 0.9498806682577565\n",
      "loss test: 0.15108850210178554 acc test: 0.8444444444444444\n",
      "loss train: 0.09826974630864733 acc train: 0.9522673031026253\n",
      "loss test: 0.15023136934963802 acc test: 0.8444444444444444\n",
      "loss train: 0.09692389094626724 acc train: 0.9530628480509149\n",
      "loss test: 0.1494111049835609 acc test: 0.8481481481481481\n",
      "loss train: 0.09555600682472927 acc train: 0.9538583929992045\n",
      "loss test: 0.14865069148337295 acc test: 0.8481481481481481\n",
      "loss train: 0.09420271986754924 acc train: 0.9562450278440732\n",
      "loss test: 0.14793122828865976 acc test: 0.8481481481481481\n",
      "loss train: 0.0929051146425578 acc train: 0.9578361177406524\n",
      "loss test: 0.14722076490364674 acc test: 0.8481481481481481\n",
      "loss train: 0.09164059831194572 acc train: 0.9578361177406524\n",
      "loss test: 0.14651753691763988 acc test: 0.8481481481481481\n",
      "loss train: 0.09040831232822331 acc train: 0.9578361177406524\n",
      "loss test: 0.14585330275321667 acc test: 0.8537037037037037\n",
      "loss train: 0.08920693031420325 acc train: 0.958631662688942\n",
      "loss test: 0.14525307394234058 acc test: 0.8555555555555555\n",
      "loss train: 0.08802373441648668 acc train: 0.9610182975338106\n",
      "loss test: 0.14470425279144755 acc test: 0.8574074074074074\n",
      "loss train: 0.08685476724956005 acc train: 0.9610182975338106\n",
      "loss test: 0.14419159008615132 acc test: 0.8592592592592593\n",
      "loss train: 0.08571112792875522 acc train: 0.9610182975338106\n",
      "loss test: 0.14368642514190433 acc test: 0.8611111111111112\n",
      "loss train: 0.08461223025630234 acc train: 0.9634049323786794\n",
      "loss test: 0.14323562523713088 acc test: 0.8629629629629629\n",
      "loss train: 0.08352256323772178 acc train: 0.9634049323786794\n",
      "loss test: 0.1428805833591884 acc test: 0.8648148148148148\n",
      "loss train: 0.08245436101270635 acc train: 0.9634049323786794\n",
      "loss test: 0.14239619076020182 acc test: 0.8629629629629629\n",
      "loss train: 0.08148205457012313 acc train: 0.964200477326969\n",
      "loss test: 0.1418011041498273 acc test: 0.8666666666666667\n",
      "loss train: 0.08055585796350152 acc train: 0.9665871121718377\n",
      "loss test: 0.1411964225554411 acc test: 0.8666666666666667\n",
      "loss train: 0.07965624229107339 acc train: 0.9673826571201273\n",
      "loss test: 0.14059094399347527 acc test: 0.8722222222222222\n",
      "loss train: 0.07877735317862757 acc train: 0.9681782020684169\n",
      "loss test: 0.13997120438487282 acc test: 0.8722222222222222\n",
      "loss train: 0.07791826715576071 acc train: 0.9697692919649961\n",
      "loss test: 0.13932104908176463 acc test: 0.8740740740740741\n",
      "loss train: 0.0770797153857136 acc train: 0.9705648369132857\n",
      "loss test: 0.13864401761752795 acc test: 0.8777777777777778\n",
      "loss train: 0.07626006944168512 acc train: 0.9721559268098647\n",
      "loss test: 0.13796543847643203 acc test: 0.8777777777777778\n",
      "loss train: 0.07545736582352082 acc train: 0.9721559268098647\n",
      "loss test: 0.13731536904797007 acc test: 0.8777777777777778\n",
      "loss train: 0.07467253659168502 acc train: 0.9737470167064439\n",
      "loss test: 0.13670264267574606 acc test: 0.8777777777777778\n",
      "loss train: 0.07390471572762457 acc train: 0.9745425616547335\n",
      "loss test: 0.136119164833098 acc test: 0.8814814814814815\n",
      "loss train: 0.07315020624184512 acc train: 0.9761336515513126\n",
      "loss test: 0.1355589223148526 acc test: 0.8814814814814815\n",
      "loss train: 0.0724063291876382 acc train: 0.9769291964996022\n",
      "loss test: 0.13501686644548883 acc test: 0.8851851851851852\n",
      "loss train: 0.0716719593612357 acc train: 0.9777247414478918\n",
      "loss test: 0.13448203107399606 acc test: 0.8851851851851852\n",
      "loss train: 0.07094590333886867 acc train: 0.9785202863961814\n",
      "loss test: 0.13394014670040766 acc test: 0.8851851851851852\n",
      "loss train: 0.07022653772280633 acc train: 0.9785202863961814\n",
      "loss test: 0.13338550366279 acc test: 0.8851851851851852\n",
      "loss train: 0.06951393631335269 acc train: 0.979315831344471\n",
      "loss test: 0.13283345053913406 acc test: 0.8888888888888888\n",
      "loss train: 0.0688127825505396 acc train: 0.979315831344471\n",
      "loss test: 0.13231460285848382 acc test: 0.8888888888888888\n",
      "loss train: 0.06813417666287724 acc train: 0.9801113762927606\n",
      "loss test: 0.13184074358271136 acc test: 0.8888888888888888\n",
      "loss train: 0.06748365431505941 acc train: 0.9801113762927606\n",
      "loss test: 0.1313974352198777 acc test: 0.8888888888888888\n",
      "loss train: 0.06685422660912223 acc train: 0.9801113762927606\n",
      "loss test: 0.13097260938594318 acc test: 0.8888888888888888\n",
      "loss train: 0.06623787997722594 acc train: 0.9809069212410502\n",
      "loss test: 0.13056196520885252 acc test: 0.8907407407407407\n",
      "loss train: 0.06562819784023276 acc train: 0.9809069212410502\n",
      "loss test: 0.1301652247660825 acc test: 0.8907407407407407\n",
      "loss train: 0.06501986668314784 acc train: 0.9809069212410502\n",
      "loss test: 0.12978420080104952 acc test: 0.8925925925925926\n",
      "loss train: 0.06440781686008794 acc train: 0.9817024661893397\n",
      "loss test: 0.1294240356468159 acc test: 0.8944444444444445\n",
      "loss train: 0.0637875490096403 acc train: 0.9840891010342084\n",
      "loss test: 0.1290953465066481 acc test: 0.8944444444444445\n",
      "loss train: 0.0631595969083708 acc train: 0.984884645982498\n",
      "loss test: 0.12879917896208876 acc test: 0.8944444444444445\n",
      "train completed!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    Y_pred = []\n",
    "    for x, y in zip(X_train, Y_train):\n",
    "\n",
    "        # forward\n",
    "        x = x.reshape(-1, 1)\n",
    "        net1 = x.T @ W1 + B1\n",
    "        out1 = sigmoid(net1)\n",
    "        net2 = out1 @ W2 + B2\n",
    "        out2 = sigmoid(net2)\n",
    "        net3 = out2 @ W3 + B3\n",
    "        out3 = softmax(net3)\n",
    "        y_pred = out3\n",
    "        Y_pred.append(y_pred.T)\n",
    "\n",
    "        # back propagation\n",
    "        error = -2 * (y - y_pred)\n",
    "        grad_W3 = out2.T @ error\n",
    "        grad_B3 = error\n",
    "        error = error @ W3.T * out2 * (1 - out2)\n",
    "        grad_W2 = out1.T @ error\n",
    "        grad_B2 = error\n",
    "        error = error @ W2.T * out1 * (1 - out1)\n",
    "        grad_W1 = x @ error\n",
    "        grad_B1 = error\n",
    "\n",
    "        # update\n",
    "        W1 = W1 - Î· * grad_W1\n",
    "        B1 = B1 - Î· * grad_B1\n",
    "        W2 = W2 - Î· * grad_W2\n",
    "        B2 = B2 - Î· * grad_B2\n",
    "        W3 = W3 - Î· * grad_W3\n",
    "        B3 = B3 - Î· * grad_B3\n",
    "\n",
    "    Y_pred = np.array(Y_pred).reshape(-1, 10)\n",
    "    loss_train = root_mean_squired_error(Y_pred, Y_train)\n",
    "    acc_train = np.mean(np.argmax(Y_pred, axis=1) == np.argmax(Y_train, axis=1))\n",
    "    \n",
    "    # test\n",
    "\n",
    "    Y_pred = []\n",
    "    for x, y in zip(X_test, Y_test):\n",
    "\n",
    "        # forward\n",
    "        x = x.reshape(-1, 1)\n",
    "\n",
    "        # layer 1\n",
    "        net1 = x.T @ W1 + B1\n",
    "        out1 = sigmoid(net1)\n",
    "\n",
    "        # layer 2\n",
    "        net2 = out1 @ W2 + B2\n",
    "        out2 = sigmoid(net2)\n",
    "\n",
    "        # layer 3\n",
    "        net3 = out2 @ W3 + B3\n",
    "        out3 = softmax(net3)\n",
    "\n",
    "        y_pred = out3\n",
    "        Y_pred.append(y_pred.T)\n",
    "\n",
    "    Y_pred = np.array(Y_pred).reshape(-1, 10)\n",
    "    loss_test = root_mean_squired_error(Y_pred, Y_test)\n",
    "    acc_test = np.mean(np.argmax(Y_pred, axis=1) == np.argmax(Y_test, axis=1))\n",
    "\n",
    "    print('loss train:', loss_train, 'acc train:', acc_train)\n",
    "    print('loss test:', loss_test, 'acc test:', acc_test)\n",
    "\n",
    "print('train completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\98915\\AppData\\Local\\Temp\\ipykernel_27232\\2389740904.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread(\"test.png\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "image = image.astype(np.float32)\n",
    "\n",
    "x = image.reshape(-1, 1)\n",
    "\n",
    "# layer 1\n",
    "net1 = x.T @ W1 + B1\n",
    "out1 = sigmoid(net1)\n",
    "\n",
    "# layer 2\n",
    "net2 = out1 @ W2 + B2\n",
    "out2 = sigmoid(net2)\n",
    "\n",
    "# layer 3\n",
    "net3 = out2 @ W3 + B3\n",
    "out3 = softmax(net3)\n",
    "\n",
    "y_pred = out3\n",
    "print(np.argmax(y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
